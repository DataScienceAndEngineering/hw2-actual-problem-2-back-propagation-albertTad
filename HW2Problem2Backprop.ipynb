{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW2: Problem 2: Working out Backpropagation\n\nRead Chapter 2 of Michael Nielsen's article/book from top to bottom:\n\n* [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)\n\nHe outlines a few exersizes in that article which you must complete. Do the following a, b, c:\n\na. He invites you to write out a proof of equation BP3\n\nb. He invites you to write out a proof of equation BP4\n\nc. He proposes that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$.\n\nYour submission should be a single jupyter notebook. Use markdown cells with latex for equations of a jupyter notebook for each proof for \"a.\" and \"b.\". Make sure you include text that explains your steps. Next for \"c\" this is an implementation problem. You need to understand and modify the code the Michael Nielsen provided so that instead it is using a matrixed based approach. Again don't keep the biases separate. After reading data in (use the iris data set), create a new column corresponding to $x_0=1$, and as mentioned above and discussed in class (see notes) is that the bias term can then be considered a weight $w_0$. Again use markdown cells around your code and comments to explain your work. Test the code on the iris data set with 4 node input (5 with a constant 1), three hidden nodes, and three output nodes, one for each species/class.","metadata":{"id":"xuuA_SHMuUne"}},{"cell_type":"markdown","source":"## a. Proof of Michael Nielsons equation BP3\n\n## Proof of BP3\n\nTo prove equation BP3, we start with the definition of the error $\\delta^l$ for layer $l$:\n\n\\[\n\\delta^l = \\nabla_a C \\odot \\sigma'(z^l)\n\\]\n\nwhere $\\nabla_a C$ is the gradient of the cost function with respect to the activations, and $\\sigma'(z^l)$ is the derivative of the activation function.\n\nRecall that the activation $a^l$ is given by:\n\n\\[\na^l = \\sigma(z^l)\n\\]\n\nTherefore, the derivative of $a^l$ with respect to $z^l$ is:\n\n\\[\n\\frac{\\partial a^l}{\\partial z^l} = \\sigma'(z^l)\n\\]\n\nSubstituting the activation function derivative into the chain rule, we get:\n\n\\[\n\\frac{\\partial C}{\\partial z^l} = \\frac{\\partial C}{\\partial a^l} \\odot \\sigma'(z^l)\n\\]\n\nBy definition of $\\delta^l$:\n\n\\[\n\\delta^l = \\frac{\\partial C}{\\partial z^l} = \\frac{\\partial C}{\\partial a^l} \\odot \\sigma'(z^l)\n\\]\n\nFor the backpropagation step from layer $l+1$ to $l$, we have:\n\n\\[\n\\frac{\\partial C}{\\partial a^l} = (w^{l+1})^T \\delta^{l+1}\n\\]\n\nSubstituting this into the definition of $\\delta^l$, we get:\n\n\\[\n\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l)\n\\]\n\nThus, equation BP3 is proven.\n","metadata":{"id":"NsfIQN1-uwGB"}},{"cell_type":"markdown","source":"## b. Proof of Michael Nielsons equation BP4\n\n## Proof of BP4\n\nTo prove equation BP4, we start with the definition of the partial derivative of the cost with respect to the weights:\n\n\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n\\]\n\nwhere $a_k^{l-1}$ is the activation from the previous layer, and $\\delta_j^l$ is the error term for the current layer.\n\nRecall that $\\delta_j^l$ represents the error in the activations $a^l$ at neuron $j$ in layer $l$. We need to show that:\n\n\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n\\]\n\nProof:\n1. From the chain rule, we have:\n\n\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = \\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial w_{jk}^l}\n\\]\n\n2. Recall that $z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$, thus:\n\n\\[\n\\frac{\\partial z_j^l}{\\partial w_{jk}^l} = a_k^{l-1}\n\\]\n\n3. Substituting into the chain rule, we get:\n\n\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = \\frac{\\partial C}{\\partial z_j^l} a_k^{l-1}\n\\]\n\n4. By definition of $\\delta_j^l$:\n\n\\[\n\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l}\n\\]\n\n5. Substituting $\\delta_j^l$ into the partial derivative expression, we get:\n\n\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l\n\\]\n\nThus, equation BP4 is proven.","metadata":{"id":"axsx4eu9u_3x"}},{"cell_type":"markdown","source":"## c. Using both markdown cells and code cells implement that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$.","metadata":{"id":"rj1HbN9RvVXe"}},{"cell_type":"code","source":"# Code cell for part c.","metadata":{"id":"3wUek4Nau7x7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# One-hot encode the labels\nencoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse\ny = encoder.fit_transform(y.reshape(-1, 1))\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Add bias term (column of ones)\nX = np.hstack([X, np.ones((X.shape[0], 1))])\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RcWxpEGb1W9G","outputId":"ebea55b8-9ec8-4508-8b55-7e8b5adfd924","execution":{"iopub.status.busy":"2024-05-17T04:40:04.235449Z","iopub.execute_input":"2024-05-17T04:40:04.235929Z","iopub.status.idle":"2024-05-17T04:40:04.249121Z","shell.execute_reply.started":"2024-05-17T04:40:04.235902Z","shell.execute_reply":"2024-05-17T04:40:04.248282Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"d6Jv_j1WveF9"}},{"cell_type":"code","source":"class NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights with small random values\n        self.w1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.w2 = np.random.randn(hidden_size, output_size) * 0.01\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def sigmoid_prime(self, z):\n        return self.sigmoid(z) * (1 - self.sigmoid(z))\n\n    def forward(self, X):\n        self.z1 = np.dot(X, self.w1)\n        self.a1 = self.sigmoid(self.z1)\n        self.z2 = np.dot(self.a1, self.w2)\n        self.a2 = self.sigmoid(self.z2)\n        return self.a2\n\n    def backward(self, X, y, output):\n        m = X.shape[0]\n        \n        # Calculate the output error\n        self.output_error = output - y\n        self.output_delta = self.output_error * self.sigmoid_prime(self.z2)\n        \n        # Calculate the hidden layer error\n        self.z1_error = np.dot(self.output_delta, self.w2.T)\n        self.z1_delta = self.z1_error * self.sigmoid_prime(self.z1)\n        \n        # Update weights\n        self.w2 -= np.dot(self.a1.T, self.output_delta) / m\n        self.w1 -= np.dot(X.T, self.z1_delta) / m\n\n    def train(self, X, y, epochs=1000, learning_rate=0.1):\n        for epoch in range(epochs):\n            output = self.forward(X)\n            self.backward(X, y, output)\n            \n    def predict(self, X):\n        output = self.forward(X)\n        return np.argmax(output, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T04:40:23.084981Z","iopub.execute_input":"2024-05-17T04:40:23.085354Z","iopub.status.idle":"2024-05-17T04:40:23.098786Z","shell.execute_reply.started":"2024-05-17T04:40:23.085319Z","shell.execute_reply":"2024-05-17T04:40:23.097811Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Initialize and train the neural network\nnn = NeuralNetwork(input_size=5, hidden_size=3, output_size=3)\nnn.train(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T04:40:30.254999Z","iopub.execute_input":"2024-05-17T04:40:30.255816Z","iopub.status.idle":"2024-05-17T04:40:30.381595Z","shell.execute_reply.started":"2024-05-17T04:40:30.255784Z","shell.execute_reply":"2024-05-17T04:40:30.380549Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Predict on the test set\npredictions = nn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T04:40:36.051187Z","iopub.execute_input":"2024-05-17T04:40:36.051531Z","iopub.status.idle":"2024-05-17T04:40:36.055882Z","shell.execute_reply.started":"2024-05-17T04:40:36.051505Z","shell.execute_reply":"2024-05-17T04:40:36.054856Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Calculate accuracy\naccuracy = np.mean(predictions == np.argmax(y_test, axis=1))\nprint(f'Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T04:40:38.275938Z","iopub.execute_input":"2024-05-17T04:40:38.276287Z","iopub.status.idle":"2024-05-17T04:40:38.282155Z","shell.execute_reply.started":"2024-05-17T04:40:38.276261Z","shell.execute_reply":"2024-05-17T04:40:38.280774Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Accuracy: 96.67%\n","output_type":"stream"}]}]}